{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GraphGenerator(nn.Module):\n",
    "#     def __init__(self, latent_dim, node_dim, bond_type_dim, atomic_number_dim):\n",
    "#         super(GraphGenerator, self).__init__()\n",
    "#         self.node_dim = node_dim\n",
    "#         self.bond_type_dim = bond_type_dim\n",
    "#         self.atomic_number_dim = atomic_number_dim\n",
    "\n",
    "\n",
    "#         self.fc_layers = nn.Sequential(\n",
    "#             nn.Linear(latent_dim, 1024),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(1024, 2048),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(2048, 4096),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "#         self.bond_type_head = nn.Linear(4096, node_dim * node_dim * bond_type_dim)\n",
    "\n",
    "#         self.distance_head = nn.Linear(4096, node_dim * node_dim)\n",
    "\n",
    "#         self.atomic_number_head = nn.Linear(4096, node_dim * atomic_number_dim)\n",
    "\n",
    "#     def forward(self, z):\n",
    "#         x = self.fc_layers(z)\n",
    "\n",
    "#         bond_type_logits = self.bond_type_head(x).view(-1, self.node_dim, self.node_dim, self.bond_type_dim)\n",
    "#         bond_type_probs = F.softmax(bond_type_logits, dim=-1)\n",
    "\n",
    "\n",
    "#         distance_logits = self.distance_head(x).view(-1, self.node_dim, self.node_dim, 1)\n",
    "#         distances = torch.sigmoid(distance_logits) \n",
    "\n",
    "#         atomic_number_logits = self.atomic_number_head(x).view(-1, self.node_dim, self.atomic_number_dim)\n",
    "#         atomic_number_probs = F.softmax(atomic_number_logits, dim=-1)\n",
    "\n",
    "#         return bond_type_probs, distances, atomic_number_probs\n",
    "\n",
    "# class GraphDiscriminator(nn.Module):\n",
    "#     def __init__(self, node_dim, bond_type_dim, atomic_number_dim):\n",
    "#         super(GraphDiscriminator, self).__init__()\n",
    "#         self.node_dim = node_dim\n",
    "#         self.bond_type_dim = bond_type_dim\n",
    "#         self.atomic_number_dim = atomic_number_dim\n",
    "\n",
    "#         self.conv_layers = nn.Sequential(\n",
    "#             nn.Conv2d(bond_type_dim, 32, kernel_size=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(32, 64, kernel_size=1),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "#         self.fc_layers = nn.Sequential(\n",
    "#             nn.Linear(64 * node_dim * node_dim + node_dim * atomic_number_dim, 1024),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(1024, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(512, 1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, bond_type_probs, distances, atomic_number_probs):\n",
    "\n",
    "#         bond_type_probs = bond_type_probs.view(-1, self.bond_type_dim, self.node_dim, self.node_dim)\n",
    "\n",
    "#         conv_out = self.conv_layers(bond_type_probs)\n",
    "#         conv_out = conv_out.view(-1, 64 * self.node_dim * self.node_dim)\n",
    "\n",
    "#         atomic_number_probs = atomic_number_probs.view(-1, self.node_dim * self.atomic_number_dim)\n",
    "\n",
    "#         combined_features = torch.cat([conv_out, atomic_number_probs], dim=1)\n",
    "\n",
    "#         validity = self.fc_layers(combined_features)\n",
    "#         return validity\n",
    "class GraphGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim, node_dim, bond_type_dim, atomic_number_dim):\n",
    "        super(GraphGenerator, self).__init__()\n",
    "        self.node_dim = node_dim\n",
    "        self.bond_type_dim = bond_type_dim\n",
    "        self.atomic_number_dim = atomic_number_dim\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.Dropout(0.3),  # Introduce dropout\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 4096),\n",
    "            nn.Dropout(0.3),  # Introduce dropout\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.bond_type_head = nn.Linear(4096, node_dim * node_dim * bond_type_dim)\n",
    "        self.distance_head = nn.Linear(4096, node_dim * node_dim)\n",
    "        self.atomic_number_head = nn.Linear(4096, node_dim * atomic_number_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.fc_layers(z)\n",
    "        bond_type_probs = F.softmax(self.bond_type_head(x).view(-1, self.node_dim, self.node_dim, self.bond_type_dim), dim=-1)\n",
    "        distances = torch.sigmoid(self.distance_head(x).view(-1, self.node_dim, self.node_dim, 1))\n",
    "        atomic_number_probs = F.softmax(self.atomic_number_head(x).view(-1, self.node_dim, self.atomic_number_dim), dim=-1)\n",
    "        return bond_type_probs, distances, atomic_number_probs\n",
    "\n",
    "class GraphDiscriminator(nn.Module):\n",
    "    def __init__(self, node_dim, bond_type_dim, atomic_number_dim):\n",
    "        super(GraphDiscriminator, self).__init__()\n",
    "        self.node_dim = node_dim\n",
    "        self.bond_type_dim = bond_type_dim + 1  # Update to account for the distance channel\n",
    "        self.atomic_number_dim = atomic_number_dim\n",
    "\n",
    "        # Initialize convolution layers for processing the combined bond types and distances\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(self.bond_type_dim, 32, kernel_size=1),  # Adjusted for bond_type_dim + 1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers for final validity prediction\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(64 * node_dim * node_dim + node_dim * atomic_number_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, bond_type_probs, distances, atomic_number_probs):\n",
    "        # Concatenate bond_type_probs and distances along the last dimension (channels)\n",
    "        combined_input = torch.cat([bond_type_probs, distances], dim=-1)  # now shape [batch, 63, 63, 5]\n",
    "\n",
    "        # Apply convolutional layers to the combined tensor\n",
    "        conv_out = self.conv_layers(combined_input.permute(0, 3, 1, 2))  # Rearrange to [batch, channels, height, width]\n",
    "        conv_out_flat = conv_out.view(-1, 64 * self.node_dim * self.node_dim)\n",
    "\n",
    "        # Flatten atomic number probabilities and concatenate with convolution output\n",
    "        atomic_number_probs_flat = atomic_number_probs.view(-1, self.node_dim * self.atomic_number_dim)\n",
    "        combined_features = torch.cat([conv_out_flat, atomic_number_probs_flat], dim=1)\n",
    "\n",
    "        # Compute the validity score\n",
    "        validity = self.fc_layers(combined_features)\n",
    "        return validity\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "atomic_number_tensors = torch.load('../data/atomic_number_tensors.pt')\n",
    "bond_tensors = torch.load('../data/bond_tensors.pt')\n",
    "distance_tensors = torch.load('../data/distance_tensors.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_atomic_numbers(atomic_num_tensor):\n",
    "    \"\"\"\n",
    "    Count the frequency of each atomic number across all molecules and positions.\n",
    "\n",
    "    Args:\n",
    "    atomic_num_tensor (torch.Tensor): Tensor of shape (n_molecules, num_atoms, max_atomic_number)\n",
    "                                      where each slice along the second dimension is a one-hot encoded atomic number.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with atomic number as keys and counts as values.\n",
    "    \"\"\"\n",
    "    # Sum across molecules and positions to get the total count of each atomic number\n",
    "    atomic_counts = torch.sum(atomic_num_tensor, dim=(0, 1))\n",
    "\n",
    "    # Create a dictionary to hold atomic number counts\n",
    "    atomic_number_counts = {}\n",
    "    for i, count in enumerate(atomic_counts, start=1):  # starting index is 1 if atomic numbers start at 1\n",
    "        atomic_number_counts[i] = int(count.item())\n",
    "\n",
    "    return atomic_number_counts\n",
    "\n",
    "def count_molecules_with_atomic_numbers(atomic_num_tensor):\n",
    "    \"\"\"\n",
    "    Count how many molecules contain each atomic number at least once.\n",
    "\n",
    "    Args:\n",
    "    atomic_num_tensor (torch.Tensor): Tensor of shape (n_molecules, num_atoms, max_atomic_number)\n",
    "                                      where each slice along the second dimension is a one-hot encoded atomic number.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with atomic number as keys and the number of molecules containing that atomic number as values.\n",
    "    \"\"\"\n",
    "    # Check if an atomic number is present in each molecule by summing over the position dimension and checking if greater than zero\n",
    "    presence_matrix = torch.sum(atomic_num_tensor, dim=1) > 0\n",
    "\n",
    "    # Sum over all molecules to count how many have each atomic number\n",
    "    molecular_counts = torch.sum(presence_matrix, dim=0)\n",
    "\n",
    "    # Create a dictionary to hold the counts of molecules with each atomic number\n",
    "    molecule_counts_by_atomic_number = {}\n",
    "    for i, count in enumerate(molecular_counts, start=1):  # starting index is 1 if atomic numbers start at 1\n",
    "        molecule_counts_by_atomic_number[i] = int(count.item())\n",
    "\n",
    "    return molecule_counts_by_atomic_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_counts = count_atomic_numbers(atomic_number_tensors)\n",
    "m_counts = count_molecules_with_atomic_numbers(atomic_number_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 630]' is invalid for input of size 217728",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39m# Train Discriminator with smoothed labels\u001b[39;00m\n\u001b[1;32m     44\u001b[0m optimizer_dis\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 45\u001b[0m real_pred \u001b[39m=\u001b[39m discriminator(bond_batch, distance_batch, atomic_number_batch)\n\u001b[1;32m     46\u001b[0m fake_pred \u001b[39m=\u001b[39m discriminator(fake_bonds\u001b[39m.\u001b[39mdetach(), fake_distances\u001b[39m.\u001b[39mdetach(), fake_atomic_numbers\u001b[39m.\u001b[39mdetach())\n\u001b[1;32m     47\u001b[0m real_loss \u001b[39m=\u001b[39m criterion(real_pred, smooth_real_labels)\n",
      "File \u001b[0;32m~/anaconda3/envs/epi/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/epi/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[82], line 138\u001b[0m, in \u001b[0;36mGraphDiscriminator.forward\u001b[0;34m(self, bond_type_probs, distances, atomic_number_probs)\u001b[0m\n\u001b[1;32m    135\u001b[0m conv_out_flat \u001b[39m=\u001b[39m conv_out\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m64\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_dim \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_dim)\n\u001b[1;32m    137\u001b[0m \u001b[39m# Flatten atomic number probabilities and concatenate with convolution output\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m atomic_number_probs_flat \u001b[39m=\u001b[39m atomic_number_probs\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_dim \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39matomic_number_dim)\n\u001b[1;32m    139\u001b[0m combined_features \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([conv_out_flat, atomic_number_probs_flat], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    141\u001b[0m \u001b[39m# Compute the validity score\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 630]' is invalid for input of size 217728"
     ]
    }
   ],
   "source": [
    "latent_dim = 256\n",
    "num_epochs = 25\n",
    "batch_size = 64\n",
    "learning_rate = 0.0001\n",
    "node_dim = 63\n",
    "bond_type_dim = 4\n",
    "atomic_number_dim = 10\n",
    "\n",
    "# Instantiate models\n",
    "generator = GraphGenerator(latent_dim, node_dim, bond_type_dim, atomic_number_dim)\n",
    "discriminator = GraphDiscriminator(node_dim, bond_type_dim, atomic_number_dim)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_gen = torch.optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "optimizer_dis = torch.optim.Adam(discriminator.parameters(), lr=learning_rate*.001)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    for i in range(0, len(bond_tensors), batch_size):\n",
    "        # Ensure not exceeding data size\n",
    "        batch_end = min(i + batch_size, len(bond_tensors))\n",
    "        bond_batch = bond_tensors[i:batch_end]\n",
    "        distance_batch = distance_tensors[i:batch_end]\n",
    "        atomic_number_batch = atomic_number_tensors[i:batch_end]\n",
    "        \n",
    "        # Generate random latent vectors\n",
    "        z = torch.randn((batch_end - i, latent_dim))\n",
    "\n",
    "        # Generate fake data with generator\n",
    "        fake_bonds, fake_distances, fake_atomic_numbers = generator(z)\n",
    "        # print(fake_bonds.shape)\n",
    "        # print(fake_distances.shape)\n",
    "        # print(fake_atomic_numbers.shape)\n",
    "\n",
    "        # Label smoothing\n",
    "        smooth_factor = 0.1  # Adjust as needed\n",
    "        smooth_real_labels = (1.0 - smooth_factor) * torch.ones((batch_end - i, 1))\n",
    "        smooth_fake_labels = smooth_factor * torch.ones((batch_end - i, 1))\n",
    "\n",
    "        # Train Discriminator with smoothed labels\n",
    "        optimizer_dis.zero_grad()\n",
    "        real_pred = discriminator(bond_batch, distance_batch, atomic_number_batch)\n",
    "        fake_pred = discriminator(fake_bonds.detach(), fake_distances.detach(), fake_atomic_numbers.detach())\n",
    "        real_loss = criterion(real_pred, smooth_real_labels)\n",
    "        fake_loss = criterion(fake_pred, smooth_fake_labels)\n",
    "        dis_loss = (real_loss + fake_loss) / 2\n",
    "        dis_loss.backward()\n",
    "        optimizer_dis.step()\n",
    "        \n",
    "        # # Train Discriminator: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        # optimizer_dis.zero_grad()\n",
    "        # real_pred = discriminator(bond_batch, distance_batch, atomic_number_batch)\n",
    "        # fake_pred = discriminator(fake_bonds.detach(), fake_distances.detach(), fake_atomic_numbers.detach())\n",
    "        # real_loss = criterion(real_pred, torch.ones_like(real_pred))\n",
    "        # fake_loss = criterion(fake_pred, torch.zeros_like(fake_pred))\n",
    "        # dis_loss = (real_loss + fake_loss) / 2\n",
    "        # dis_loss.backward()\n",
    "        # optimizer_dis.step()\n",
    "\n",
    "        # Train Generator: minimize log(1 - D(G(z))) or maximize log(D(G(z)))\n",
    "        optimizer_gen.zero_grad()\n",
    "        gen_pred = discriminator(fake_bonds, fake_distances, fake_atomic_numbers)\n",
    "        gen_loss = criterion(gen_pred, torch.ones_like(gen_pred))\n",
    "        gen_loss.backward()\n",
    "        optimizer_gen.step()\n",
    "\n",
    "        # Logging or print statements for loss\n",
    "        if i % 100 == 0:  # Adjust print frequency according to your preference\n",
    "            print(f\"Epoch {epoch}, Batch {i}, Loss D: {dis_loss.item()}, Loss G: {gen_loss.item()}\")\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss D: 0.6955199241638184, Loss G: 0.6844384074211121\n",
      "Epoch 0, Batch 1600, Loss D: 0.8009901642799377, Loss G: 0.4761163592338562\n",
      "Epoch 0, Batch 3200, Loss D: 0.7765430212020874, Loss G: 0.5121188163757324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [01:00<24:13, 60.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0, Loss D: 0.774576723575592, Loss G: 0.5155914425849915\n",
      "Epoch 1, Batch 1600, Loss D: 0.7725940942764282, Loss G: 0.518730878829956\n",
      "Epoch 1, Batch 3200, Loss D: 0.7719885110855103, Loss G: 0.519282877445221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [01:56<22:06, 57.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 0, Loss D: 0.7720956802368164, Loss G: 0.5193299651145935\n",
      "Epoch 2, Batch 1600, Loss D: 0.7721900343894958, Loss G: 0.5193670988082886\n",
      "Epoch 2, Batch 3200, Loss D: 0.7719211578369141, Loss G: 0.5193856954574585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3/25 [02:48<20:17, 55.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch 0, Loss D: 0.7720561027526855, Loss G: 0.5193907022476196\n",
      "Epoch 3, Batch 1600, Loss D: 0.7721812129020691, Loss G: 0.5193809270858765\n",
      "Epoch 3, Batch 3200, Loss D: 0.7719161510467529, Loss G: 0.5193937420845032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 4/25 [03:38<18:34, 53.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch 0, Loss D: 0.7720508575439453, Loss G: 0.5193993449211121\n",
      "Epoch 4, Batch 1600, Loss D: 0.7721786499023438, Loss G: 0.5193850994110107\n",
      "Epoch 4, Batch 3200, Loss D: 0.7719229459762573, Loss G: 0.519382655620575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5/25 [04:25<17:02, 51.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch 0, Loss D: 0.7720550298690796, Loss G: 0.5193924307823181\n",
      "Epoch 5, Batch 1600, Loss D: 0.7721781134605408, Loss G: 0.5193859338760376\n",
      "Epoch 5, Batch 3200, Loss D: 0.7719231843948364, Loss G: 0.5193821787834167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [05:13<15:48, 49.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Batch 0, Loss D: 0.7720510363578796, Loss G: 0.5193989872932434\n",
      "Epoch 6, Batch 1600, Loss D: 0.7721767425537109, Loss G: 0.5193881392478943\n",
      "Epoch 6, Batch 3200, Loss D: 0.7719215154647827, Loss G: 0.5193849802017212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7/25 [06:03<14:56, 49.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Batch 0, Loss D: 0.7720509171485901, Loss G: 0.5193991661071777\n",
      "Epoch 7, Batch 1600, Loss D: 0.7721723914146423, Loss G: 0.5193952918052673\n",
      "Epoch 7, Batch 3200, Loss D: 0.7719195485115051, Loss G: 0.5193881988525391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 8/25 [06:59<14:41, 51.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Batch 0, Loss D: 0.7720432877540588, Loss G: 0.5194116830825806\n",
      "Epoch 8, Batch 1600, Loss D: 0.7721790075302124, Loss G: 0.5193846225738525\n",
      "Epoch 8, Batch 3200, Loss D: 0.771902322769165, Loss G: 0.5194163918495178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 9/25 [07:55<14:12, 53.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Batch 0, Loss D: 0.7720521092414856, Loss G: 0.5193973183631897\n",
      "Epoch 9, Batch 1600, Loss D: 0.7721748948097229, Loss G: 0.5193912386894226\n",
      "Epoch 9, Batch 3200, Loss D: 0.7719144821166992, Loss G: 0.5193964242935181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 10/25 [08:44<12:57, 51.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Batch 0, Loss D: 0.7720596790313721, Loss G: 0.5193848609924316\n",
      "Epoch 10, Batch 1600, Loss D: 0.7721790075302124, Loss G: 0.519384503364563\n",
      "Epoch 10, Batch 3200, Loss D: 0.7719216346740723, Loss G: 0.5193847417831421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 11/25 [09:34<11:57, 51.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Batch 0, Loss D: 0.7720673084259033, Loss G: 0.5193723440170288\n",
      "Epoch 11, Batch 1600, Loss D: 0.772171676158905, Loss G: 0.5193964838981628\n",
      "Epoch 11, Batch 3200, Loss D: 0.771918535232544, Loss G: 0.519389808177948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 12/25 [10:55<13:04, 60.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Batch 0, Loss D: 0.7720555663108826, Loss G: 0.5193915963172913\n",
      "Epoch 12, Batch 1600, Loss D: 0.772176206111908, Loss G: 0.5193890929222107\n",
      "Epoch 12, Batch 3200, Loss D: 0.7719260454177856, Loss G: 0.5193775296211243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 13/25 [11:50<11:43, 58.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Batch 0, Loss D: 0.7720587253570557, Loss G: 0.5193864703178406\n",
      "Epoch 13, Batch 1600, Loss D: 0.7721744775772095, Loss G: 0.5193918347358704\n",
      "Epoch 13, Batch 3200, Loss D: 0.7719194293022156, Loss G: 0.5193884372711182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 13/25 [12:40<11:41, 58.49s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m fake_loss \u001b[39m=\u001b[39m criterion(fake_pred, smooth_fake_labels)\n\u001b[1;32m     45\u001b[0m dis_loss \u001b[39m=\u001b[39m (real_loss \u001b[39m+\u001b[39m fake_loss) \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m---> 46\u001b[0m dis_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     47\u001b[0m optimizer_dis\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     49\u001b[0m \u001b[39m# Train Generator\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/epi/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/epi/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[39m.\u001b[39m_execution_engine\u001b[39m.\u001b[39mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "latent_dim = 256\n",
    "num_epochs = 25\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "node_dim = 63\n",
    "bond_type_dim = 4\n",
    "atomic_number_dim = 54\n",
    "\n",
    "# Instantiate models\n",
    "generator = GraphGenerator(latent_dim, node_dim, bond_type_dim, atomic_number_dim)\n",
    "discriminator = GraphDiscriminator(node_dim, bond_type_dim, atomic_number_dim)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_gen = torch.optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "optimizer_dis = torch.optim.Adam(discriminator.parameters(), lr=learning_rate*.01)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "scheduler_gen = StepLR(optimizer_gen, step_size=10, gamma=0.05)\n",
    "scheduler_dis = StepLR(optimizer_dis, step_size=10, gamma=0.05)\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    for i in range(0, len(bond_tensors), batch_size):\n",
    "        batch_end = min(i + batch_size, len(bond_tensors))\n",
    "        bond_batch = bond_tensors[i:batch_end]\n",
    "        distance_batch = distance_tensors[i:batch_end]\n",
    "        atomic_number_batch = atomic_number_tensors[i:batch_end]\n",
    "        \n",
    "        z = torch.randn((batch_end - i, latent_dim))\n",
    "\n",
    "        # Generate fake data with generator\n",
    "        fake_bonds, fake_distances, fake_atomic_numbers = generator(z)\n",
    "        \n",
    "        # Label smoothing for stability\n",
    "        smooth_real_labels = 0.9 * torch.ones((batch_end - i, 1))\n",
    "        smooth_fake_labels = 0.1 * torch.ones((batch_end - i, 1))\n",
    "\n",
    "        # Train Discriminator with smoothed labels\n",
    "        optimizer_dis.zero_grad()\n",
    "        real_pred = discriminator(bond_batch, distance_batch, atomic_number_batch)\n",
    "        fake_pred = discriminator(fake_bonds.detach(), fake_distances.detach(), fake_atomic_numbers.detach())\n",
    "        real_loss = criterion(real_pred, smooth_real_labels)\n",
    "        fake_loss = criterion(fake_pred, smooth_fake_labels)\n",
    "        dis_loss = (real_loss + fake_loss) / 2\n",
    "        dis_loss.backward()\n",
    "        optimizer_dis.step()\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_gen.zero_grad()\n",
    "        gen_pred = discriminator(fake_bonds, fake_distances, fake_atomic_numbers)\n",
    "        gen_loss = criterion(gen_pred, torch.ones_like(gen_pred))\n",
    "        gen_loss.backward()\n",
    "        optimizer_gen.step()\n",
    "\n",
    "        # Update learning rate schedules\n",
    "        scheduler_gen.step()\n",
    "        scheduler_dis.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {i}, Loss D: {dis_loss.item()}, Loss G: {gen_loss.item()}\")\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_parameters(model, print_all=False):\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        param = parameter.numel()\n",
    "        if print_all:\n",
    "            print(f\"{name}: {parameter.size()} -> {param} parameters\")\n",
    "        total_params += param\n",
    "    print(f\"Total trainable parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 105838287\n",
      "Total trainable parameters: 264124641\n"
     ]
    }
   ],
   "source": [
    "print_model_parameters(generator)\n",
    "print_model_parameters(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
